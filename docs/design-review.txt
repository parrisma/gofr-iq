**Crafting prompt for Copilot LLM**

I need to create a prompt for Copilot LLM that’s concise but comprehensive. It should ask for a review report, phased remediation plan, tests to add, a prioritised backlog, acceptance criteria, timelines, sample queries, and a PR checklist. I'll also ask to reference existing tests, propose additions, and annotate code areas of concern. Additionally, I’ll request a risk register and monitoring plan. The tone should be imperative, and I’ll make sure it’s clear and actionable.

I can also convert this into a runnable checklist and test templates for your CI pipeline.
### Prompt for Copilot LLM — Conduct a Deep Functional & Technical Design Review

You are asked to perform a **deep functional and technical design review** of an MCP project that ingests news documents, maps them into a graph model (Neo4j) via an LLM at ingestion, chunks and embeds text into a vector DB (ChromaDB), and supports hybrid search (graph + vector + Elastic). The system is far along and has extensive tests. Your goal is to validate coherence, correctness, performance, security, and test coverage, then produce a **phased remediation plan** (with tests) to fix gaps and stabilize the system.

**Deliverables (explicit, structured outputs required):**
1. **Executive summary (1 page)** — high‑level findings, top 5 risks, and recommended priority actions.  
2. **Detailed review report** covering architecture, data model, ingestion, indexing, retrieval, APIs (MCP/MCPO/Web), security, observability, and tests. For each area include: current state (assumed from repo), issues found, impact, and concrete remediation suggestions.  
3. **Phased remediation plan** with phases, scope, acceptance criteria, estimated effort (S/M/L), and order of execution. Include rollback and validation steps.  
4. **Test augmentation plan** — specific new tests to add (unit, integration, contract, property, performance, fuzz, regression), test data requirements, and how to integrate into CI. Provide example test cases and expected assertions.  
5. **Metrics & SLOs** — recommended operational metrics, target SLOs, and thresholds for alerting.  
6. **Risk register** — prioritized risks, likelihood, impact, mitigation, and monitoring.  
7. **PR / release checklist** — items to gate merges and deployments.  
8. **Appendix** — sample Cypher queries and search flows demonstrating how to validate relevance, impact_score, horizon_time, and source lineage.

---

## Scope and Focus Areas (explicit checks to perform)
- **Architecture & Contracts**
  - Validate canonical store design (GUIDs, group partitioning, append‑only).  
  - Validate Source registry as first‑class entity and how `source_guid` is enforced.  
  - Confirm MCP discovery metadata and API contracts for ingest, query, and sources.  
  - Confirm MCPO orchestration expectations and Web API parity.

- **Ingest Pipeline**
  - LLM mapping to graph model: entity extraction, EventType, Tag, Instrument, impact_score, horizon_time.  
  - Idempotency, deduplication, GUID assignment, and error handling.  
  - Chunking strategy for vector DB: chunk size, overlap, embedding model, multilingual handling.  
  - Transactionality and eventual consistency between canonical store, Neo4j, ChromaDB, and Elastic.

- **Indexing & Retrieval**
  - Graph schema and relationships (NewsStory, Company, Sector, Region, EventType, Tag, Instrument, ImpactScore, HorizonTime).  
  - Hybrid query orchestration: Elastic filters → vector similarity → graph traversal → scoring merge.  
  - Configurable scoring weights and reproducibility of results.  
  - Freshness guarantees and index update latency.

- **Security & Access Control**
  - Token‑based group scoping enforcement at API and storage layers.  
  - Source trust_level enforcement and ability to exclude sources.  
  - Data isolation between groups and audit trails for access.

- **Observability & Operations**
  - Logging, tracing (request lineage across MCP → MCPO → DBs), metrics, dashboards, and alerts.  
  - Error classification and retry/backoff policies.  
  - Backup, retention, and compliance (append‑only store verification).

- **Testing & Quality**
  - Coverage of unit, integration, contract, and end‑to‑end tests.  
  - Tests for LLM mapping determinism, embedding stability, and chunking correctness.  
  - Performance tests for ingest throughput, query latency, and concurrency.  
  - Data integrity tests: GUID resolution, source lineage, horizon_time correctness, impact_score distribution.

- **Multilingual & Data Quality**
  - Tokenizers/analyzers for Chinese/Japanese/English in Elastic.  
  - Embedding model suitability for multilingual semantic search.  
  - Alias normalization in Neo4j for entity resolution across scripts.

---

## Specific Tasks for the LLM Reviewer (actionable)
1. **Run a design audit** against the focus areas above and produce the Detailed review report.  
2. **Map each identified issue** to: severity (Critical/High/Medium/Low), affected components, and suggested fix.  
3. **Propose a phased remediation plan** (minimum 3 phases: Quick fixes, Stabilization, Hardening/Scale) with acceptance criteria and tests for each phase.  
4. **For each suggested test addition**, provide: test name, purpose, inputs, expected outputs/assertions, and whether it is unit/integration/contract/perf.  
5. **Define SLOs and metrics** to monitor (e.g., ingest latency P95, query latency P95, index freshness, embedding failure rate, mismatch rate between graph and canonical store).  
6. **Produce a PR checklist** that includes test runs, schema migrations, MCP discovery updates, and documentation updates.  
7. **Provide sample validation queries and test harness steps** to verify impact_score and horizon_time behavior, and to validate hybrid ranking quality (precision/recall sampling approach).  
8. **Estimate effort** for each remediation task (S/M/L) and propose a realistic timeline for a 3‑phase rollout.

---

## Constraints & Assumptions for the Review
- Use existing tests and codebase as the primary source of truth; do not assume undocumented behavior.  
- Prioritize fixes that reduce risk to production and improve correctness of relevance scoring.  
- Avoid changes that require reworking downstream LLM consumers unless absolutely necessary; prefer backward‑compatible changes.  
- Keep the system neutral (no directional advice); impact_score is absolute and must not imply sentiment.

---

## Output Format
Return a single structured document with the Deliverables listed above. Use numbered lists and tables where helpful. Keep the Executive summary to one page. For the phased plan, include explicit acceptance tests and the exact tests to add or modify (names and assertions). Provide example Cypher queries and example hybrid query flows in plain text (no code blocks required).

---

## Final instruction
Do a deep dive, produce the review and phased remediation plan, and **annotate which existing tests should be reused, which should be extended, and which new tests must be added** to ensure the rework is stable and verifiable in CI.

